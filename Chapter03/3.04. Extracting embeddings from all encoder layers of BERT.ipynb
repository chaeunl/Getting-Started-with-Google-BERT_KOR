{"cells":[{"cell_type":"markdown","metadata":{"id":"zLbXrzA0_ndC"},"source":["# BERT의 모든 인코더 레이어에서 임베딩 추출하기\n","지난 장에서 사전 학습된 BERT로부터 임베딩을 추출하는 방법에 대해 알아 보았습니다. 마지막 인더 레이어로부터 임베딩을 추출한다고 배웠습니다. 이제 생기는 의문은 오직 마지막 인코더 레이어(마지막 은닉 상태)로부터 얻은 임베딩만을 고려해야 될까요? 아니면, 모든 인코더 레이어(모든 은닉 상태)로부터 얻은 임베딩을 고려해야 될까요? 이 점에 대해 좀 더 살펴보겠습니다.\n","\n","아래 그림과 같이, 입력 임베딩 레이어를 $h_0$라고 하고 첫번째 레이어(첫번째 은닉층)을 $h_1$, 두번째 인코더 레이어(두번째 은닉층)를 $h_2$이라 하고 계속 진행하여 마지막 12번째 인코더 레이어를 $h_{12}$라고 표시하겠습니다:\n","\n","\n","![title](images/4.png)\n","\n","\n","마지막 인코더 레이어로부터만 임베딩(표현)을 가져오는 대신에, BERT의 저자는 다른 인코더 레이어들로부터 임베딩을 가져오는 실험을 하였습니다.\n","\n","예를 들어, named-entity recognition task에서 저자는 사전 학습된 BERT를 특징을 추출하는데 사용하였습니다. 마지막 인코더 레이어(마지막 은닉층)로부터 얻은 임베딩을 특징(feature)으로 사용하는 대신에, 다른 인코더 레이어들(다른 은닉층들)로부터 얻은 임베딩을 특징으로 사용하였고 아래의 F1 점수를 얻었습니다:\n","\n","\n","![title](images/5.png)\n","\n","위 테이블에서 알 수 있듯이, 마지막 4개의 인코더 레이어(마지막 4개의 은닉층)으로부터 얻은 임베딩을 연쇄적으로 잇는 것이 더 좋은 F1 점수(96점)를 보여줬습니다. 1% in the NER task. 그러므로, 마지막 인코더 레이어(마지막 은닉층)로부터만 임베딩을 얻기보다 다른 인코더 레이어들로부터 얻은 임베딩들 역시 사용할 수 있음을 알 수 있습니다.\n","\n","이제, 트랜스포머 라이브러리를 활용하여 모든 인코더 레이어로부터 임베딩을 추출하는 방법에 대해 알아보겠습니다.\n","\n","## 임베딩 추출하기\n","첫째로, 필요한 모듈을 불러 옵니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"A9uDL3-i_rG-"},"outputs":[],"source":["%%capture \n","#!pip install transformers==3.5.1\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"KSwq5tr9_ndJ"},"outputs":[],"source":["from transformers import BertModel, BertTokenizer\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"oI91BJ8H_ndJ"},"source":["이제, 사전 학습된 BERT 모델과 토큰나이저를 다운로드 합니다. 사전 학습된 BERT 모델을 다운로드하면서 확인할 수 있듯이, output_hidden_state를 True로 설정해야 됩니다. 이렇게 설정함으로써, 모든 인코더 레이어로부터 임베딩들을 가져올 수 있습니다:\n","\n","<font color=\"blue\">[역자]</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["9ba62db0115d4930990c3c6d087989c1","7003e521a65b49a9b9727f3f1bfa293f","94dc71fe23e040ceae5df4c4dd573684","26c1ceb389ad46ddbf841770d07a5e93","84ce79c32f354db9ad3d19acd02625c1","7706547847d7465299292bb77f2dff25","f769dd1c71324598b9e27be922759bab","2bdc25c58d624b3594fc76892aed8e09","d70b8c6e81774b3aa9e737d28be1dd48","437d981ba7c14c39b8cf0e62ee9e055e","b9b4261f0dc74e9a9210df1ec4273e6a","7c7c261fe4ac435c856fcfd568b4e298","4fdbc441590f4e4abc99b64b98169f8a","b260f036b5b141188688b837a4845790","0cc09eca07734399a4d7716f3911d7bd","f5f075e096554be9a389edbd8cf1e089","95e442aacd1e4aa5b361569341870f47","eb6a67dab3534dae96aaafa975ef3465","7bf00e5dbb204358a0a9985b914db244","a043bae8118f439ca6d5b5a9c818d6b6","a808c5be6f344f3db86cf6e3d26ec81c","e0a7468cea174a7597f30e619fb99bcb","f4060c1c44144dfa9a367d8614708b45","ed2edfd213544216a1edf723e587b3be"]},"id":"eM0UkRBp_ndK","outputId":"213cd2ae-471c-421f-e5a8-0f82a77009ee"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ba62db0115d4930990c3c6d087989c1","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d70b8c6e81774b3aa9e737d28be1dd48","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95e442aacd1e4aa5b361569341870f47","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{"id":"z6lcm9mn_ndK"},"source":["다음으로, 모델에 입력을 넣기 전에 입력 데이터를 전처리합니다.\n","\n","## 입력 전처리하기\n","이전 장에서 본 것과 같은 문장으로 진행합니다. 먼저, 문장을 토큰화하고 [CLS] 토큰을 문장의 시작에 그리고 [SEP] 토큰을 문장의 끝에 추가합니다:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1-hLzFu6_ndL"},"outputs":[],"source":["sentence = 'I love Paris'\n","tokens = tokenizer.tokenize(sentence)\n","tokens = ['[CLS]'] + tokens + ['[SEP]']"]},{"cell_type":"markdown","metadata":{"id":"YUKVAdmT_ndL"},"source":["토큰의 전체 길이를 7로 유지한다고 가정합니다. 따라서, [PAD] 토큰을 추가해야되고 attention mask 또한 정의해야 됩니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"5OISI_-t_ndM"},"outputs":[],"source":["tokens = tokens + ['[PAD]'] + ['[PAD]']\n","attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]"]},{"cell_type":"markdown","metadata":{"id":"ic5yVHnB_ndM"},"source":["다음으로, tokens를 token_ids로 변환합니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8YtAw5Gr_ndM"},"outputs":[],"source":["token_ids = tokenizer.convert_tokens_to_ids(tokens)"]},{"cell_type":"markdown","metadata":{"id":"-MmpLWP6_ndN"},"source":["이제, token_ids와 attention_mask를 텐서로 변환합니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lm7Nqj6w_ndN"},"outputs":[],"source":["token_ids = torch.tensor(token_ids).unsqueeze(0)\n","attention_mask = torch.tensor(attention_mask).unsqueeze(0)"]},{"cell_type":"markdown","metadata":{"id":"_FQ7RzxU_ndN"},"source":["다음으로 입력 데이터를 전처리하여, 임베딩을 얻습니다.\n","\n","## 임베딩 얻기\n","모델의 모든 인코더 레이어로부터 임베딩을 얻도록 모델을 정의하면서 output_hidden_states를 True로 설정하였기에, 아래에서 보는 바와 같이 모델은 세가지 값을 출력값으로 반환합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"DhmqjLG1_ndN"},"outputs":[],"source":["last_hidden_state, pooler_output, hidden_states = model(token_ids, attention_mask = attention_mask)"]},{"cell_type":"markdown","metadata":{"id":"BjN7Ojnh_ndO"},"source":["위 코드를 자세히 살펴보면 아래와 같습니다:\n","\n","첫번째 값인 last_hidden_state는 오직 마지막 인코더 레이어(인코더 12)로부터 얻은 모든 토큰의 표현(<font color=\"blu\">[역자]</font>임베딩 표현. 이 예제에선 사이즈가 (1,512,7))을 포함합니다. \n","다음으로, pooler_output은 선형 변환과 tanh 활성화함수에 의해 후처리된 마지막 인코더 레이어로부터 얻은 [CLS] 토큰에 대한 표현을 의미합니다.\n","hidden_states는 모든 인코더 레이어의 마지막 레이어로부터 얻은 모든 토큰에 대한 표현을 포함합니다(<font color=\"blu\">[역자]</font>즉, 각 인코더 레이어의 임베딩을 하나로 묶은 것 입니다)\n","이제, 각각의 값들을 살펴보고 자세히 알아보겠습니다.\n","\n","last_hidden_state 먼저 살펴보겠습니다. 앞서 말한 바와 같이, 이 반환값은 오직 마지막 인코더 레이어(인코더 1)에서 얻은 모든 토큰의 표현을 갖고 있습니다. last_hidden_state의 사이즈를 출력해봅니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlmKmXrO_ndO","outputId":"5408b910-3a71-48f5-e0d6-5afadae3a8b1"},"outputs":[{"data":{"text/plain":["torch.Size([1, 7, 768])"]},"execution_count":9,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["last_hidden_state.shape"]},{"cell_type":"markdown","metadata":{"id":"sCSrepOZ_ndO"},"source":["사이즈 [1, 7, 768]은 각각 [batch_size, sequence_length, hidden_size]를 의미합니다.\n","\n","우리가 사용한 문장의 배치 사이즈는 1이고, 문장의 길이는 토큰의 길이와 같으므로 7개의 토큰이 현재 문장에 있어 총 길이는 7입니다. 그리고, hidden_size는 표현(임베딩)의 사이즈이므로 BERT-base 모델에서 이 값은 768입니다.\n","\n","각 토큰의 임베딩을 얻을 수 있습니다:\n","- last_hidden[0][0]은 문장의 첫번째 토큰인 [CLS] 토큰에 대한 표현입니다.\n","- last_hidden[0][1]은 문장의 두번째 토큰인 'I' 토큰에 대한 표현입니다.\n","- last_hidden[0][2]은 문장의 세번째 토큰인 'love' 토큰에 대한 표현입니다.\n","\n","이와 유사하게, 마지막 인코더 레이어로 부터 모든 토큰에 대한 표현을 얻을 수 있습니다.\n","\n","다음으로, 선형 변환과 tanh 활성화함수에 의해 후처리된 마지막 인코더 레이어로부터 얻은 [CLS] 토큰에 대한 표현을 함축하는 pooler_output이 있습니다. pooler_output의 사이즈를 출력해봅니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lK5MrEuW_ndP","outputId":"bd3f4085-12f4-4b52-d73a-6a866ec8860c"},"outputs":[{"data":{"text/plain":["torch.Size([1, 768])"]},"execution_count":10,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["pooler_output.shape"]},{"cell_type":"markdown","metadata":{"id":"ERlJpFL3_ndP"},"source":["사이즈 [1,768]은 각각 [batch_size, hidden_size]를 의미합니다.\n","\n","[CLS] 토큰이 문장의 전체적인 표현을 함축하고 있다는 것을 배웠습니다. 그러므로, pooler_output을 주어진 문장인 'I love Paris'의 표현(임베딩)으로 사용할 수 있습니다.\n","\n","마지막으로, hidden_states는 모든 인코더 레이어의 마지막 층으로부터 얻는 토큰에 대한 표현(임베딩)입니다. 이 반환값은 입력 임베딩 레이어부터 마지막 인코더레이어까지 모든 인코더 레이어(은닉층)의 마지막 레이어의 표현(임베딩)을 갖고 있는 13개의 값으로 이루어진 튜플입니다. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yzF8Qtxu_ndP","outputId":"58848c36-f41f-451f-afc6-3de66a3c2663"},"outputs":[{"data":{"text/plain":["13"]},"execution_count":11,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["len(hidden_states)"]},{"cell_type":"markdown","metadata":{"id":"-ZBdQEeI_ndP"},"source":["위에서 알 수 있듯이, 이 반환값은 모든 인코더 레이어로부터 얻은 13개의 토큰 표현값을 갖고 있습니다. 따라서:\n","\n","- hidden_sttaes[0]는 입력 임베딩 레이어로부터 얻은 모든 토큰의 표현(임베딩)을 포함합니다.\n","- hidden_sttaes[1]은 첫번째 인코더 레이어로부터 얻은 모든 토큰의 표현(임베딩)을 포함합니다.\n","- hidden_sttaes[2]은 두번째 인코더 레이어로부터 얻은 모든 토큰의 표현(임베딩)을 포함합니다.\n","\n","유사하게, hidden_states[12]는 마지막 인코더 레이어로부터 얻은 모든 토큰의 표현(임베딩)을 포함합니다.\n","이에 대해 더 살펴보겟습니다. 먼저, 입력 임베딩 레이어로부터 얻은 모든 토큰에 대한 표현을 갖고 있는 hidden_states[0]의 사이즈를 출력해봅니다:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUT57PxO_ndQ","outputId":"8a8be1de-9357-4e87-8837-e4a6b425149d"},"outputs":[{"data":{"text/plain":["torch.Size([1, 7, 768])"]},"execution_count":12,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["hidden_states[0].shape"]},{"cell_type":"markdown","metadata":{"id":"PPc_0HL3_ndQ"},"source":["사이즈 [1, 7, 768]은 각각 [batch_size, sequence_length, hidden_size]를 의미합니다.\n","\n","이제, 첫번째 인코더 레이어로부터 얻은 모든 토큰에 대한 표현을 갖고 있는 hidden_states[1]의 사이즈를 출려해봅니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yGnFpzH_ndR","outputId":"7e984ae0-b575-4703-d17a-37a1a5819823"},"outputs":[{"data":{"text/plain":["torch.Size([1, 7, 768])"]},"execution_count":13,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["torch.Size([1, 7, 768])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xy_0LBQi_ndR","outputId":"d8e2da0c-e37a-4953-a1af-3a34f8ab0622"},"outputs":[{"data":{"text/plain":["torch.Size([1, 7, 768])"]},"execution_count":14,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["hidden_states[1].shape"]},{"cell_type":"markdown","metadata":{"id":"46j8VT75_ndR"},"source":["이런 방식으로, 모든 인코더 레이어로부터 얻은 토큰에 대한 표현을 얻을 수 있습니다. 이로써, 사전 학습된 BERT를 사용하여 임베딩을 추출하는 방법을 알아보았는데, sentiment analysis와 같은 다운스트림 작업에서 사전 학습된 BERT를 사용할 수 있을까요? 답은 \"Yes!\" 입니다. 이에 대해 다음 장에서 배워보도록 합시다"]}],"metadata":{"colab":{"name":"3.04. Extracting embeddings from all encoder layers of BERT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0cc09eca07734399a4d7716f3911d7bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""},"model_module_version":"1.5.0"},"26c1ceb389ad46ddbf841770d07a5e93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bdc25c58d624b3594fc76892aed8e09","placeholder":"​","style":"IPY_MODEL_f769dd1c71324598b9e27be922759bab","value":" 433/433 [00:14&lt;00:00, 28.9B/s]"},"model_module_version":"1.5.0"},"2bdc25c58d624b3594fc76892aed8e09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"437d981ba7c14c39b8cf0e62ee9e055e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"4fdbc441590f4e4abc99b64b98169f8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"},"model_module_version":"1.5.0"},"7003e521a65b49a9b9727f3f1bfa293f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"7706547847d7465299292bb77f2dff25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"7bf00e5dbb204358a0a9985b914db244":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_e0a7468cea174a7597f30e619fb99bcb","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a808c5be6f344f3db86cf6e3d26ec81c","value":231508},"model_module_version":"1.5.0"},"7c7c261fe4ac435c856fcfd568b4e298":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5f075e096554be9a389edbd8cf1e089","placeholder":"​","style":"IPY_MODEL_0cc09eca07734399a4d7716f3911d7bd","value":" 440M/440M [00:11&lt;00:00, 39.1MB/s]"},"model_module_version":"1.5.0"},"84ce79c32f354db9ad3d19acd02625c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"},"model_module_version":"1.5.0"},"94dc71fe23e040ceae5df4c4dd573684":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_7706547847d7465299292bb77f2dff25","max":433,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84ce79c32f354db9ad3d19acd02625c1","value":433},"model_module_version":"1.5.0"},"95e442aacd1e4aa5b361569341870f47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bf00e5dbb204358a0a9985b914db244","IPY_MODEL_a043bae8118f439ca6d5b5a9c818d6b6"],"layout":"IPY_MODEL_eb6a67dab3534dae96aaafa975ef3465"},"model_module_version":"1.5.0"},"9ba62db0115d4930990c3c6d087989c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94dc71fe23e040ceae5df4c4dd573684","IPY_MODEL_26c1ceb389ad46ddbf841770d07a5e93"],"layout":"IPY_MODEL_7003e521a65b49a9b9727f3f1bfa293f"},"model_module_version":"1.5.0"},"a043bae8118f439ca6d5b5a9c818d6b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed2edfd213544216a1edf723e587b3be","placeholder":"​","style":"IPY_MODEL_f4060c1c44144dfa9a367d8614708b45","value":" 232k/232k [00:00&lt;00:00, 805kB/s]"},"model_module_version":"1.5.0"},"a808c5be6f344f3db86cf6e3d26ec81c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"},"model_module_version":"1.5.0"},"b260f036b5b141188688b837a4845790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"b9b4261f0dc74e9a9210df1ec4273e6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_b260f036b5b141188688b837a4845790","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fdbc441590f4e4abc99b64b98169f8a","value":440473133},"model_module_version":"1.5.0"},"d70b8c6e81774b3aa9e737d28be1dd48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9b4261f0dc74e9a9210df1ec4273e6a","IPY_MODEL_7c7c261fe4ac435c856fcfd568b4e298"],"layout":"IPY_MODEL_437d981ba7c14c39b8cf0e62ee9e055e"},"model_module_version":"1.5.0"},"e0a7468cea174a7597f30e619fb99bcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"eb6a67dab3534dae96aaafa975ef3465":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"ed2edfd213544216a1edf723e587b3be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"f4060c1c44144dfa9a367d8614708b45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""},"model_module_version":"1.5.0"},"f5f075e096554be9a389edbd8cf1e089":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"f769dd1c71324598b9e27be922759bab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""},"model_module_version":"1.5.0"}}}},"nbformat":4,"nbformat_minor":0}